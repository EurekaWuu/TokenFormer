# TokenFormer


<div align="center">
  <img src="assets/Figure1.png" width="800"/>
</div>

## é¡¹ç›®ç®€ä»‹

TokenFormeræ˜¯ä¸€ä¸ªå¼€åˆ›æ€§çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå°†æ¨¡å‹å‚æ•°è¿›è¡ŒtokenåŒ–å®ç°äº†**å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶**çš„è®¡ç®—æ¡†æ¶ã€‚

- **å‚æ•°tokenåŒ–**ï¼šé¦–æ¬¡å°†æ¨¡å‹å‚æ•°è½¬æ¢ä¸ºtokenï¼Œä¸è¾“å…¥æ•°æ®tokenåœ¨åŒä¸€æ³¨æ„åŠ›ç©ºé—´ä¸­äº¤äº’
- **Pattentionæœºåˆ¶**ï¼šæå‡ºäº†å‚æ•°æ³¨æ„åŠ›ï¼ˆParameter Attentionï¼‰æœºåˆ¶ï¼Œç»Ÿä¸€å¤„ç†token-tokenå’Œtoken-parameteräº¤äº’  
- **åŸç”Ÿå¯æ‰©å±•**ï¼šæ”¯æŒæ¸è¿›å¼æ¨¡å‹æ‰©å±•ï¼Œæ— éœ€ä»å¤´é‡æ–°è®­ç»ƒå¤§æ¨¡å‹
- **æ¶æ„ç»Ÿä¸€**ï¼štokenåŒ–å®ç°äº†æ•°æ®ã€å‚æ•°å’Œè®°å¿†çš„ç»Ÿä¸€è¡¨ç¤º


## é¡¹ç›®æ¶æ„

```
TokenFormer/
â”œâ”€â”€ configs/                    # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ tokenformer/           # TokenFormerä¸“ç”¨é…ç½®
â”‚   â”œâ”€â”€ llama/                 # LLaMAæ¨¡å‹é…ç½®
â”‚   â””â”€â”€ incremental_scaling/   # å¢é‡æ‰©å±•é…ç½®
â”œâ”€â”€ megatron/                  # æ ¸å¿ƒæ¡†æ¶
â”‚   â”œâ”€â”€ model/                 # æ¨¡å‹å®ç°
â”‚   â”‚   â”œâ”€â”€ tokenformer.py    # TokenFormerä¸»è¦å®ç°
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ training.py           # è®­ç»ƒé€»è¾‘
â”‚   â””â”€â”€ ...
â”œâ”€â”€ eval_tasks/               # è¯„ä¼°ä»»åŠ¡
â”œâ”€â”€ tools/                    # å·¥å…·è„šæœ¬
â”œâ”€â”€ requirements/             # ä¾èµ–ç®¡ç†
â”œâ”€â”€ train.py                  # è®­ç»ƒå…¥å£
â”œâ”€â”€ eval.py                   # è¯„ä¼°å…¥å£
â””â”€â”€ generate.py              # æ¨ç†å…¥å£
```

## ç¯å¢ƒå®‰è£…

### ç³»ç»Ÿè¦æ±‚

- **Python**: 3.8 
- **CUDA**: 12.0+ 
- **PyTorch**: 2.2.1+
- **GPU**: æ”¯æŒNVIDIA GPUï¼ˆå•å¡æˆ–å¤šå¡ï¼‰

### å®‰è£…æ­¥éª¤

1. **åˆ›å»ºç¯å¢ƒ**
```bash
conda create -n TokenFormer python=3.8
conda activate TokenFormer
```

2. **å…‹éš†é¡¹ç›®**
```bash
git clone https://github.com/Haiyang-W/TokenFormer.git
cd TokenFormer
```

3. **å®‰è£…PyTorch**
```bash
pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121
```

4. **å®‰è£…ä¾èµ–**
```bash
# åŸºç¡€ä¾èµ–
pip install -r requirements/requirements.txt

# å¯é€‰ç»„ä»¶
pip install -r requirements/requirements-flashattention.txt  # éœ€è¦ GCC > 9
pip install -r requirements/requirements-wandb.txt          # WandBæ—¥å¿—
pip install -r requirements/requirements-tensorboard.txt    # TensorBoardæ—¥å¿—
```

5. **å®‰è£…APEX**
```bash
git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation \
    --config-settings "--build-option=--cpp_ext" \
    --config-settings "--build-option=--cuda_ext" ./
```

## å¼€å§‹

### æ¨¡å‹æ¨ç†

1. **ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹**
```bash
# ä»HuggingFaceä¸‹è½½
git lfs clone https://huggingface.co/Haiyang-W/TokenFormer-150M
```

2. **é›¶æ ·æœ¬è¯„ä¼°**
```bash
python ./deepy.py eval.py -d configs tokenformer/150M_eval.yml \
    --eval_tasks lambada_openai hellaswag piqa arc_challenge arc_easy winogrande
```

3. **æ–‡æœ¬ç”Ÿæˆ**
```bash
python ./deepy.py generate.py -d configs tokenformer/150M_eval.yml \
    --text_gen_type input-file \
    --sample_input_file input.txt \
    --sample_output_file output.txt
```

### æ•°æ®å‡†å¤‡

1. **å‡†å¤‡å°å‹æ•°æ®é›†ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰**
```bash
python prepare_data.py -d ./data -t HFTokenizer --vocab-file tokenizer.json openwebtext2
```

2. **å‡†å¤‡Pileæ•°æ®é›†ï¼ˆå®Œæ•´è®­ç»ƒï¼‰**
```bash
python prepare_data.py -d ./data -t HFTokenizer --vocab-file tokenizer.json pile
```

### æ¨¡å‹è®­ç»ƒ

1. **å•èŠ‚ç‚¹è®­ç»ƒ**
```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
python deepy.py train.py configs/tokenformer/150M_train_pile.yml
```

2. **å¤šèŠ‚ç‚¹è®­ç»ƒï¼ˆSlurmï¼‰**
```bash
# ä¿®æ”¹é…ç½®æ–‡ä»¶
vim configs/tokenformer/150M_train_pile.yml
# è®¾ç½® "launcher": "slurm", "deepspeed_slurm": true

```

### å¢é‡æ‰©å±•è®­ç»ƒ

```bash
# ä¸‹è½½354MåŸºç¡€æ¨¡å‹
wget https://huggingface.co/Haiyang-W/TokenFormer-354M-Openwebtext2/resolve/main/354M_TokenFormer_Openwebtext2.zip
unzip 354M_TokenFormer_Openwebtext2.zip

# æ‰§è¡Œå¢é‡æ‰©å±•ï¼ˆ354M â†’ 757Mï¼‰
python deepy.py train.py configs/incremental_scaling_openwebtext2/354M_to_757M_train_openwebtext2_60k.yml
```

## æ¨¡å‹æ€§èƒ½

### é¢„è®­ç»ƒæ¨¡å‹åˆ—è¡¨

| æ¨¡å‹ | å‚æ•°é‡ | å±‚æ•° | éšè—ç»´åº¦ | ä¸‹è½½é“¾æ¥ | é…ç½®æ–‡ä»¶ |
|------|--------|------|----------|-----------|----------|
| TokenFormer-150M | 150M | 12 | 768 | [ğŸ¤— HF](https://huggingface.co/Haiyang-W/TokenFormer-150M) | [config](configs/tokenformer/150M_eval.yml) |
| TokenFormer-450M | 450M | 24 | 1024 | [ğŸ¤— HF](https://huggingface.co/Haiyang-W/TokenFormer-450M) | [config](configs/tokenformer/450M_eval.yml) |
| TokenFormer-900M | 900M | 32 | 1280 | [ğŸ¤— HF](https://huggingface.co/Haiyang-W/TokenFormer-900M) | [config](configs/tokenformer/900M_eval.yml) |
| TokenFormer-1.5B | 1.5B | 40 | 1536 | [ğŸ¤— HF](https://huggingface.co/Haiyang-W/TokenFormer-1-5B) | [config](configs/tokenformer/1-5B_eval.yml) |

### é›¶æ ·æœ¬è¯„ä¼°ç»“æœ

<div align="center">
  <img src="assets/Figure3.png" width="800"/>
</div>

### å¢é‡æ‰©å±•æ€§èƒ½

| æ¨¡å‹ | ç­–ç•¥ | è¿­ä»£æ¬¡æ•° | éªŒè¯å›°æƒ‘åº¦ | è®­ç»ƒæ•ˆç‡ |
|------|------|----------|------------|----------|
| TokenFormer-354M | ä»å¤´è®­ç»ƒ | 600k | 11.9 | åŸºå‡† |
| TokenFormer-757M | å¢é‡æ‰©å±• | 60k | 10.9 | **10å€åŠ é€Ÿ** |
| TokenFormer-757M | å¢é‡æ‰©å±• | 120k | 10.7 | **5å€åŠ é€Ÿ** |

## æŠ€æœ¯æ¶æ„

### Pattentionæœºåˆ¶

TokenFormerçš„æ ¸å¿ƒæ˜¯Pattentionï¼ˆParameter Attentionï¼‰æœºåˆ¶ï¼š

```python
# Pattentionå®ç°ç¤ºä¾‹
query, key, value = inputs, key_param_tokens, value_param_tokens

attn_weight = query @ key.transpose(-2, -1) * scale_factor
attn_weight *= attn_masks

# ä¿®æ”¹çš„å½’ä¸€åŒ–å‡½æ•°ï¼ˆéæ ‡å‡†softmaxï¼‰
attn_weight = nonlinear_norm_func(attn_weight, norm_activation_type, dim=-1)

output = attn_weight @ value
```

### å…³é”®ç‰¹æ€§

- **å‚æ•°tokenåŒ–**ï¼šå°†ä¼ ç»Ÿçš„æƒé‡çŸ©é˜µè½¬æ¢ä¸ºå¯å­¦ä¹ çš„tokenåºåˆ—
- **ç»Ÿä¸€æ³¨æ„åŠ›**ï¼šæ•°æ®tokenå’Œå‚æ•°tokenåœ¨åŒä¸€æ³¨æ„åŠ›ç©ºé—´ä¸­äº¤äº’
- **çµæ´»æ‰©å±•**ï¼šæ”¯æŒåŠ¨æ€å¢åŠ å‚æ•°tokenæ•°é‡ï¼Œå®ç°æ¨¡å‹è§„æ¨¡æ‰©å±•
- **å½’ä¸€åŒ–åˆ›æ–°**ï¼šä½¿ç”¨L2å½’ä¸€åŒ–ç­‰æ›¿ä»£ä¼ ç»Ÿsoftmaxï¼Œæå‡æ•°å€¼ç¨³å®šæ€§

## æ”¯æŒçš„ä»»åŠ¡

### è¯­è¨€å»ºæ¨¡
- **æ•°æ®é›†**: Pile, OpenWebText2, Enwik8
- **è¯„ä¼°**: å›°æƒ‘åº¦ã€é›¶æ ·æœ¬ä»»åŠ¡
- **ä»»åŠ¡**: LAMBADA, HellaSwag, PIQA, ARCç­‰

### è®¡ç®—æœºè§†è§‰
- **æ•°æ®é›†**: ImageNet-1K
- **ä»»åŠ¡**: å›¾åƒåˆ†ç±»
- **æ¶æ„**: Vision TokenFormer

### å¤šæ¨¡æ€
- **è§†è§‰-è¯­è¨€**ç†è§£
- **è·¨æ¨¡æ€**æ£€ç´¢
- **å¤šæ¨¡æ€**ç”Ÿæˆ

## é…ç½®

### é…ç½®æ–‡ä»¶è¯¦è§£

TokenFormerä½¿ç”¨YAMLæ ¼å¼çš„é…ç½®æ–‡ä»¶ï¼Œä¸»è¦åŒ…æ‹¬ï¼š

```yaml
# æ¨¡å‹æ¶æ„
num_layers: 12                    # å±‚æ•°
hidden_size: 768                  # éšè—ç»´åº¦
num_attention_heads: 12           # æ³¨æ„åŠ›å¤´æ•°
qkv_slot_num: 768                # Query/Key/Valueå‚æ•°slotæ•°é‡
proj_slot_num: 768               # æŠ•å½±å±‚å‚æ•°slotæ•°é‡
ffn_slot_num: 3072               # FFNå‚æ•°slotæ•°é‡

# TokenFormerç‰¹æœ‰è®¾ç½®
attention_config: [[["tokenformer"], 12]]
norm_activation_type: "l2_norm_gelu"    # å½’ä¸€åŒ–æ¿€æ´»ç±»å‹

# è®­ç»ƒè®¾ç½®
train_micro_batch_size_per_gpu: 32
gradient_accumulation_steps: 4
learning_rate: 0.0006
```

### åˆ†å¸ƒå¼è®­ç»ƒ

åˆ†å¸ƒå¼è®­ç»ƒæ–¹å¼ï¼š

1. **æ•°æ®å¹¶è¡Œ**: é€‚ç”¨äºå•èŠ‚ç‚¹å¤šGPU
2. **æ¨¡å‹å¹¶è¡Œ**: é€‚ç”¨äºè¶…å¤§æ¨¡å‹
3. **æµæ°´çº¿å¹¶è¡Œ**: é€‚ç”¨äºæ·±åº¦æ¨¡å‹
4. **æ··åˆå¹¶è¡Œ**: ç»„åˆä¸Šè¿°ç­–ç•¥

### ä¼˜åŒ–æŠ€å·§

- **æ¢¯åº¦æ£€æŸ¥ç‚¹**: èŠ‚çœæ˜¾å­˜
- **æ··åˆç²¾åº¦**: åŠ é€Ÿè®­ç»ƒ
- **ZeROä¼˜åŒ–**: DeepSpeedé›†æˆ
- **Flash Attention**: é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—


## å‚è€ƒèµ„æ–™

### è®ºæ–‡å¼•ç”¨

```bibtex
@inproceedings{
    wang2025tokenformer,
    title={TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters},
    author={Haiyang Wang and Yue Fan and Muhammad Ferjad Naeem and Liwei Wang and Yongqin Xian and Jan Eric Lenssen and Federico Tombari and Bernt Schiele},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=oQ4igHyh3N}
}
```

### ç›¸å…³å·¥ä½œ

- [GPT-NeoX](https://github.com/EleutherAI/gpt-neox): æœ¬é¡¹ç›®çš„åŸºç¡€æ¡†æ¶
- [Mamba](https://arxiv.org/abs/2312.00752): çŠ¶æ€ç©ºé—´æ¨¡å‹
- [TTT](https://arxiv.org/abs/2407.04620): æµ‹è¯•æ—¶é—´è®­ç»ƒ